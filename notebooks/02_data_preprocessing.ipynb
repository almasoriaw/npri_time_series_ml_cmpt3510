{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPRI Time Series Analysis - Data Preprocessing\n",
    "\n",
    "This notebook focuses on preprocessing the National Pollutant Release Inventory (NPRI) dataset merged with 2023 data from the CMPT2400 project. The preprocessing steps prepare the data for time series analysis and forecasting.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The preprocessing pipeline includes:\n",
    "\n",
    "1. Data cleaning and handling missing values\n",
    "2. Feature engineering for time series analysis\n",
    "3. Creating lag variables\n",
    "4. One-hot encoding categorical variables\n",
    "5. Preparing train/test splits for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Import utility functions (optional)\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.utils.data_utils import load_data, create_lag_features, one_hot_encode_categories\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Initial Inspection\n",
    "\n",
    "First, let's load the dataset that was previously examined in the exploration notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "df_releases = pd.read_csv('../data/raw/df_merged_releases.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df_releases.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df_releases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "Let's clean the dataset by handling missing values, removing duplicates, and addressing any data quality issues identified in the exploration phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for missing values\n",
    "missing_values = df_releases.isnull().sum()\n",
    "print(\"Missing values:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Handle missing values\n",
    "df_cleaned = df_releases.copy()\n",
    "\n",
    "# Fill missing values for release columns with 0 (assuming missing means no release)\n",
    "release_columns = ['Total_Air_Releases', 'Total_Land_Releases', 'Total_Water_Releases']\n",
    "for col in release_columns:\n",
    "    df_cleaned[col] = df_cleaned[col].fillna(0)\n",
    "\n",
    "# Handle other missing values as appropriate\n",
    "# [Add additional missing value handling logic based on exploration findings]\n",
    "\n",
    "# Drop duplicates if any\n",
    "df_cleaned = df_cleaned.drop_duplicates()\n",
    "\n",
    "# Verify cleaning results\n",
    "print(f\"\\nDataset shape after cleaning: {df_cleaned.shape}\")\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df_cleaned.isnull().sum()[df_cleaned.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Now, let's create features that will be useful for time series modeling, including lag features and time-based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create lag features for each release type\n",
    "df_with_lags = df_cleaned.copy()\n",
    "\n",
    "# Define grouping columns for creating lag features\n",
    "# (e.g., create lags within each facility-substance combination)\n",
    "group_cols = ['Facility_Name', 'Substance_Name', 'Industry_Sector']\n",
    "\n",
    "# Create lag features for each release type\n",
    "for col in release_columns:\n",
    "    df_with_lags = create_lag_features(df_with_lags, col, lag_periods=7, group_cols=group_cols)\n",
    "\n",
    "# Display the dataframe with lag features\n",
    "print(\"\\nDataframe with lag features:\")\n",
    "df_with_lags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create additional time-based features\n",
    "\n",
    "# Add year difference from baseline\n",
    "min_year = df_with_lags['Reporting_Year'].min()\n",
    "df_with_lags['Year_Diff'] = df_with_lags['Reporting_Year'] - min_year\n",
    "\n",
    "# Create cyclical features if there are seasonal patterns\n",
    "# (Not applicable if data is annual, but included as an example)\n",
    "if 'Month' in df_with_lags.columns:\n",
    "    df_with_lags['Month_Sin'] = np.sin(2 * np.pi * df_with_lags['Month'] / 12)\n",
    "    df_with_lags['Month_Cos'] = np.cos(2 * np.pi * df_with_lags['Month'] / 12)\n",
    "\n",
    "# Display added features\n",
    "print(\"\\nDataframe with time-based features:\")\n",
    "df_with_lags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Categorical Variable Encoding\n",
    "\n",
    "Next, let's encode categorical variables such as Industry_Sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = ['Industry_Sector'] \n",
    "# Add other categorical columns if needed\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "df_encoded = one_hot_encode_categories(df_with_lags, categorical_cols)\n",
    "\n",
    "# Display encoded dataframe\n",
    "print(\"\\nDataframe with one-hot encoded categories:\")\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Time Series Split Preparation\n",
    "\n",
    "For time series modeling, we need to create train/test splits that respect the chronological order of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sort data chronologically\n",
    "df_encoded = df_encoded.sort_values(by=['Reporting_Year']).reset_index(drop=True)\n",
    "\n",
    "# Create train/test split (80% train, 20% test)\n",
    "split_idx = int(len(df_encoded) * 0.8)\n",
    "train_df = df_encoded.iloc[:split_idx].copy()\n",
    "test_df = df_encoded.iloc[split_idx:].copy()\n",
    "\n",
    "print(f\"\\nTrain set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "\n",
    "# Verify year ranges in train and test sets\n",
    "print(f\"\\nTrain set year range: {train_df['Reporting_Year'].min()} - {train_df['Reporting_Year'].max()}\")\n",
    "print(f\"Test set year range: {test_df['Reporting_Year'].min()} - {test_df['Reporting_Year'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Data for Modeling\n",
    "\n",
    "Finally, let's format the data to make it ready for the modeling phase. We'll create specific datasets for each target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to prepare data for a specific target variable\n",
    "def prepare_for_modeling(df, target_col, lag_cols):\n",
    "    \"\"\"\n",
    "    Prepare data for modeling by selecting appropriate features and target.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe\n",
    "    target_col : str\n",
    "        Target column name\n",
    "    lag_cols : list\n",
    "        List of lag column names to use as features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame ready for modeling\n",
    "    \"\"\"\n",
    "    # Get categorical columns (one-hot encoded industry sectors)\n",
    "    industry_cols = [col for col in df.columns if col.startswith('Industry_Sector_')]\n",
    "    \n",
    "    # Select features and target\n",
    "    feature_cols = ['Reporting_Year', 'Year_Diff'] + lag_cols + industry_cols\n",
    "    \n",
    "    # Create modeling dataframe\n",
    "    model_df = df[feature_cols + [target_col]].copy()\n",
    "    \n",
    "    # Drop rows with missing lag values\n",
    "    model_df = model_df.dropna()\n",
    "    \n",
    "    return model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare modeling datasets for each release type\n",
    "modeling_dfs = {}\n",
    "\n",
    "for target_col in release_columns:\n",
    "    # Get lag columns for this target\n",
    "    lag_cols = [f\"{target_col}_lag_{i}\" for i in range(1, 8)]\n",
    "    \n",
    "    # Prepare train and test datasets\n",
    "    train_model_df = prepare_for_modeling(train_df, target_col, lag_cols)\n",
    "    test_model_df = prepare_for_modeling(test_df, target_col, lag_cols)\n",
    "    \n",
    "    # Store in dictionary\n",
    "    modeling_dfs[target_col] = {\n",
    "        'train': train_model_df,\n",
    "        'test': test_model_df\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nPrepared {target_col} datasets:\")\n",
    "    print(f\"Train shape: {train_model_df.shape}\")\n",
    "    print(f\"Test shape: {test_model_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data\n",
    "\n",
    "Finally, let's save the processed datasets for use in the modeling notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create processed data directory if it doesn't exist\n",
    "processed_dir = '../data/processed'\n",
    "if not os.path.exists(processed_dir):\n",
    "    os.makedirs(processed_dir)\n",
    "\n",
    "# Save each processed dataset\n",
    "for target_col, datasets in modeling_dfs.items():\n",
    "    # Create target-specific filenames\n",
    "    train_filename = f\"{target_col.lower().replace('_', '-')}_train.csv\"\n",
    "    test_filename = f\"{target_col.lower().replace('_', '-')}_test.csv\"\n",
    "    \n",
    "    # Save train and test datasets\n",
    "    datasets['train'].to_csv(os.path.join(processed_dir, train_filename), index=False)\n",
    "    datasets['test'].to_csv(os.path.join(processed_dir, test_filename), index=False)\n",
    "    \n",
    "    print(f\"Saved {target_col} datasets to {processed_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "In this notebook, we performed the following preprocessing steps:\n",
    "\n",
    "1. Data cleaning: handled missing values and removed duplicates\n",
    "2. Feature engineering: created lag variables and time-based features\n",
    "3. Categorical encoding: one-hot encoded industry sectors\n",
    "4. Time series split: created chronologically ordered train/test sets\n",
    "5. Modeling preparation: created specific datasets for each target variable\n",
    "\n",
    "The processed data is now ready for modeling in the next notebook (03_modeling_evaluation.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
